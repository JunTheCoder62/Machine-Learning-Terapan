# -*- coding: utf-8 -*-
"""Machine_Learning_Terapan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Msh88Y-VZbgZoXtJf-Sdv5XzQZk5iOB1

# Import
"""

import os
import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

import phik

# Evaluation
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Tuning
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Pipeline
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.compose import ColumnTransformer

"""# Data Loading"""

data = 'https://raw.githubusercontent.com/JunTheCoder62/Feed/refs/heads/main/stroke_prediction_dataset.csv'
df = pd.read_csv(data)
df

"""Tahapan ini dimaksudkan untuk menload data yang akan digunakn dalam model

# EDA
"""

df = df.drop(['Patient ID', 'Patient Name'], axis = 1)
df.info()

"""## Handilng Missing Value

Missing value dimaksudkan agar data yang didapat akurat dengan cara drop kolom missing value.
"""

df.isna().sum()

"""terdapat 2500 missing value"""

df = df.dropna(axis=1)

"""Dropna digunakan untuk menghilangkan/menghapus data yang kosong."""

df.isnull().sum()

"""## Handling Duplicate"""

df.duplicated().sum()

"""dalam data tidak ada duplikat sehingga tidak perlu menghaupsnya data."""

df.describe(include='object')

"""Pada perintah df.describe(include='object'), fungsi .describe() digunakan untuk menghasilkan ringkasan statistik dari kolom-kolom dalam DataFrame df. Dengan menambahkan parameter include='object', kita meminta fungsi ini untuk hanya menampilkan ringkasan statistik dari kolom-kolom bertipe objek (biasanya berisi data teks atau kategori)

## Convert Type
"""

df.info()

# df['Work Type'] = df['Work Type'].astype('category')
# df['Gender'] = df['Gender'].astype('category')
# df['Smoking Status'] = df['Gender'].astype('category')
# df['Residence Type'] = df['Residence Type'].astype('category')
# df['Alcohol Intake'] = df['Alcohol Intake'].astype('category')
# df['Physical Activity'] = df['Physical Activity'].astype('category')
# df['Family History of Stroke'] = df['Family History of Stroke'].astype('category')
# df['Dietary Habits'] = df['Dietary Habits'].astype('category')
# df['Blood Pressure Levels'] = df['Blood Pressure Levels'].astype('category')
# df['Cholesterol Levels'] = df['Cholesterol Levels'].astype('category')
# df['Diagnosis'] = df['Diagnosis'].astype('category')

df_category = ['Work Type', 'Gender', 'Smoking Status', 'Residence Type',
    'Alcohol Intake', 'Physical Activity', 'Family History of Stroke',
    'Dietary Habits', 'Blood Pressure Levels', 'Cholesterol Levels', 'Diagnosis']
df[df_category] = df[df_category].apply(lambda x: x.astype('category'))
df = df.drop('Marital Status', axis = 1)
df.info()

"""pada tahapan ini kita mengkonversi data yang dari obj menjadi categorical agar memory yang digunakan tidak terlalu besar sehingga tidak membebani system yang dijalankan.

# Business Understanding

## Univariate Analysis

Univariate Analysis merupakan tahapan untuk memahami karakteristik dasar dari variable data. seperti distribusi, nilai rata - rata, variasi, dan lain - lain.
"""

df.info()

df['Diagnosis'].value_counts(normalize=True)

"""data yang ada seimbang. tidak perlu balancing data"""

df[df['Age'] == df['Age'].min()]

df['Gender'].value_counts()

df['Gender'].value_counts().plot(kind='bar')
plt.xticks()
plt.title('Gender')

"""Male category lebih banyak memiliki Stroke

"""

df['Work Type'].value_counts()

df.groupby('Work Type')['Diagnosis'].value_counts()

"""Data ini menunjukkan jumlah orang berdasarkan jenis pekerjaan dan apakah mereka pernah mengalami stroke atau tidak. Tabel ini tersusun dengan dua kategori utama: Work Type (jenis pekerjaan) dan Diagnosis (apakah mengalami stroke atau tidak)"""

# Remove Never Worked
df = df[df['Work Type'] != 'Never Worked']
df['Work Type'].value_counts()

df.groupby('Work Type')['Diagnosis'].value_counts()

"""Data ini menunjukkan jumlah kejadian stroke dan non-stroke berdasarkan jenis pekerjaan melalui pengelompokan data menggunakan groupby dan value_counts"""

df['Smoking Status'].value_counts()

df['Smoking Status'].hist()

df['Age'].hist()

df['Diagnosis'].value_counts()

df['Diagnosis'].value_counts().plot(kind='bar')
plt.xticks()
plt.title('Stroke')

# Distribution between Age and Work Type

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x ='Age', hue='Diagnosis', kde=True)

"""Gambar Grafik diatas merupakan grafik Distribusi Age vs Diagnosis, dimana umur 20+ memiliki diagnosis stroke paling banyak.

## Multivariate Analysis

Dalam tahpan Multivariate analysis teknik analisis data yang digunakan untuk memahami hubungan antara lebih dari dua variabel secara bersamaan.
"""

df_plot1 = df[['Diagnosis', 'Work Type']].groupby('Work Type').value_counts().reset_index()
sns.barplot(data=df_plot1, x='Diagnosis', y='count', hue='Work Type')
plt.title('Stroke and Work Type')

"""Grafik gambar diatas merupak gambar grafik Stroke vs Worktype. dimana dapat dilihat bahwa sektor swasta memiliki diagnosisi terbanyak.

# Features Engineering

Feature engineering adalah proses mengubah data mentah menjadi fitur yang dapat digunakan oleh algoritma machine learning untuk meningkatkan kinerja model. Fitur-fitur ini merupakan variabel yang secara langsung digunakan dalam model untuk memprediksi target atau membuat keputusan.
"""

num_features = ['Age', 'Hypertension', 'Heart Disease', 'Stroke History', 'Stress Levels']
cat_features = ['Gender', 'Work Type', 'Residence Type', 'Smoking Status', 'Alcohol Intake', 'Physical Activity', 'Family History of Stroke', 'Dietary Habits', 'Blood Pressure Levels', 'Cholesterol Levels', 'Diagnosis']

"""num_features dan cat_features merupakan pemisahan data anta Numerical data dan Categorical data yang akan digunakan untuk menentukan fitur."""

features = cat_features[0]

counter = df[features].value_counts()
persen = 100 * df[features].value_counts(normalize = True)
temp = pd.DataFrame({'data': counter, 'persentase': persen.round(1)})
print(temp)

"""fitur pertama merupakan fitur gender, dapat dilihat bahwa pembagian data antra gender male dan female."""

counter.plot(kind='bar', title="Fitur " + features);

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""diatas merupakan grafik fitur dimana perbandingan fitur gender male dan female"""

features = cat_features[1]

counter = df[features].value_counts()
persen = 100 * df[features].value_counts(normalize = True)
temp = pd.DataFrame({'data': counter, 'persentase': persen.round(1)})
print(temp)

"""sama dengan fitur gender, fitur yang digunakan kali ini merupakan fitur Work Type."""

counter.plot(kind='bar', title="Fitur " + features);

plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

"""Dapat dilihat fitur work type memiliki 4 data, yaitu Private, self-employed, goverment job, dan Never worked. Kita drop Never Worked karena dikategorian tidak Bekerja. sehingga tidak relevan dengan fitur ini."""

df.isna().sum()

df.shape

"""diatas merupakan tahapan data setelah didrop difilter dan dan EDA"""

df.dropna(inplace=True)

"""outlier digunakan untuk rentan kuartil dalam dataset"""

Q1 = df['Body Mass Index (BMI)'].quantile(0.25)
Q3 = df['Body Mass Index (BMI)'].quantile(0.75)
IQR = Q3 - Q1

# Filter
df_filtered = df[(df['Body Mass Index (BMI)'] >= Q1 - 1.5 * IQR) & (df['Body Mass Index (BMI)'] <= Q3 + 1.5 * IQR)]
# Plot
df_filtered['Body Mass Index (BMI)'].plot(kind='box')

"""dataset tidak memiliki outliers yang terdeteksi"""

df.info()

# Correlation Matrix

df_numeric = df.select_dtypes(include=[np.number])
corr = df_numeric.corr()
plt.figure(figsize=(10, 6))
sns.heatmap(data=corr, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title(" Correlation Matrix Between Numerical Feature", size = 20)
plt.show()

"""Diatas merupak Matrik Korelasi sebelum diubah ke Numerical.

# Data Preperation

Data preparation adalah proses menyiapkan data mentah untuk digunakan dalam analisis atau pelatihan model machine learning. Ini adalah langkah penting dalam pipeline machine learning, karena kualitas data sangat memengaruhi performa model. Proses data preparation melibatkan berbagai langkah untuk memastikan bahwa data yang akan digunakan lengkap, bersih, dan dalam format yang sesuai

## Encoder Data

Dalam tahapan encoding sering kali dilakukan setelah data cleaning dan sebelum normalisasi atau feature scaling dalam pipeline persiapan data. Encoding sangat penting dalam data preparation karena banyak algoritma machine learning, seperti regresi linier atau algoritma berbasis matriks, hanya bisa bekerja dengan data numerik
"""

from imblearn.under_sampling import RandomUnderSampler

# Using OnehotEncoder

# ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform='pandas')
# df_encoder = ohe.fit_transform(df[['Gender','Diagnosis', 'Work Type', 'Residence Type', 'Smoking Status', 'Alcohol Intake', 'Physical Activity', 'Family History of Stroke', 'Dietary Habits']])
# df_encoder

"""encoder dapat menggunakan beberapa teknik seperti menggunakan OneHotEncoder. OneHotEncoder merupakan teknik mengubah data Categorical menjadi Numerical dengan memisahkan data menjadi beberapa Categori, diamana data akan diubah menjadi angak 0 atau 1"""

gender_encoder = LabelEncoder()
diagnosis_encoder = LabelEncoder()
work_type_encoder = LabelEncoder()
smoking_status_encoder = LabelEncoder()
alcohol_intake_encoder = LabelEncoder()
physical_activity_encoder = LabelEncoder()
family_history_encoder = LabelEncoder()
dietary_habits_encoder = LabelEncoder()
residence_type_encoder = LabelEncoder()
blood_pressure_encoder = LabelEncoder()
cholesterol_levels_encoder = LabelEncoder()

"""Penggunaan satu objek LabelEncoder untuk beberapa kolom dapat menyebabkan masalah karena setiap kali kita memanggil fit_transform(), LabelEncoder akan "belajar" label baru dan bisa merusak hasil encoding sebelumnya. Solusi terbaik adalah membuat instance LabelEncoder yang berbeda untuk setiap kolom atau menggunakan teknik mapping manual jika kita ingin memastikan konsistensi label."""

df['Gender'] = gender_encoder.fit_transform(df['Gender'])
df['Diagnosis'] = diagnosis_encoder.fit_transform(df['Diagnosis'])
df['Work Type'] = work_type_encoder.fit_transform(df['Residence Type'])
df['Smoking Status'] = smoking_status_encoder.fit_transform(df['Smoking Status'])
df['Alcohol Intake'] = alcohol_intake_encoder.fit_transform(df['Alcohol Intake'])
df['Physical Activity'] = physical_activity_encoder.fit_transform(df['Physical Activity'])
df['Family History of Stroke'] = family_history_encoder.fit_transform(df['Family History of Stroke'])
df['Dietary Habits'] = dietary_habits_encoder.fit_transform(df['Diagnosis'])
df['Residence Type'] = residence_type_encoder.fit_transform(df['Residence Type'])
df['Blood Pressure Levels'] = blood_pressure_encoder.fit_transform(df['Blood Pressure Levels'])
df['Cholesterol Levels'] = cholesterol_levels_encoder.fit_transform(df['Cholesterol Levels'])
df.head()

"""Teknik encoder diatas merupak teknik dimana mengubah data Categorical menjadi Numerical. dengan menggunakan LabelEncoder dari library Sklear. LabelEncoder mengubah data menjadi 1, 2, 3, ... tanpa harus memisahkan data, sehingga tetap dalam satu kolom."""

# df = pd.get_dummies(df[['Work Type', 'Residence Type', 'Smoking Status', 'Alcohol Intake', 'Physical Activity', 'Family History of Stroke', 'Dietary Habits', 'Diagnosis']], drop_first=True).astype(int)
# df.head()

"""Sama seperti OneHotEncoder, mengubah data Categorical Menjadi Numerical dengan nilai 0 atau 1."""

# df_enc = pd.concat([df, df_encoder], axis=1).drop(columns=['Gender', 'Work Type', 'Residence Type', 'Smoking Status', 'Alcohol Intake', 'Physical Activity', 'Family History of Stroke', 'Dietary Habits', 'Blood Pressure Levels', 'Cholesterol Levels'])
# df_enc.head(7)

"""dalam tahapan ini digunakan hanya untuk nilai data 0 atau 1, dikarenakan data yang diubah akan dibagi menjadi beberapa bagian."""

# df_enc.info()

corr = df.corr()
plt.figure(figsize=(10, 6))
sns.heatmap(data=corr, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title(" Correlation Matrix Between Numerical Feature", size = 20)
plt.show()

"""gambar matiks korelasi diatas merupakan matriks korelasi yang diubah dari categorical menjadi Numerical. Dapat dilihat bahwa fitur Diagnosis dan Dietary Habits dapat digunakan sebagai model."""

df.corr()

df.info()

# corr = df.corr()
# plt.figure(figsize=(10, 6))
# sns.heatmap(data=corr, annot=True, cmap='coolwarm', linewidths=0.5)
# plt.title(" Correlation Matrix Between Numerical Feature", size = 20)
# plt.show()

"""# Modelling"""

# split data test dan train
X = df.drop(['Diagnosis'], axis=1)
y = df['Diagnosis']

#  balancing Data
X_resampled, y_resampled = RandomUnderSampler().fit_resample(X,y)

# Train and Test data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,
                                                    test_size=0.2,
                                                    random_state=0)

"""Pada tahapan ini data dibagi menjadi train_data dan test_data untuk pemodelan machine learning. dan pada tahapin ini juga kita dapat melihat data overfitting, dimana model terlalu sesui dengan data latih tetapi memiliki kinerja yang buruk."""

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""diatas merupak data yang akan digunakan untuk train_data dan test_data"""

print([len(X_train), len(y_train)])
print([len(X_test), len(y_test)])

y_train.value_counts(normalize=True)

"""tahapan diatas adalah untuk menghitung proporsi dari setiap kelas yang ada dalam variabel target y_train"""

y_test.value_counts(normalize=True)

"""tahapan diatas adalah untuk menghitung proporsi dari setiap kelas yang ada dalam variabel target y_test"""

from sklearn import metrics
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix

"""import metris yang digunakan dalam data

## Normalisasi Data
"""

# scaler
scaler = MinMaxScaler()
# Normalisasi kolom 'Diagnosis'
df[['Diagnosis']] = scaler.fit_transform(df[['Diagnosis']])

# Tampilkan data yang sudah di normalisasi
print(df[['Diagnosis']].head())

"""## Random Forest"""

# RandomForest

modem_rm = RandomForestClassifier()
modem_rm.fit(X_train, y_train)
predic_rf = modem_rm.predict(X_test)

print("akurasi:", accuracy_score(y_test, predic_rf))
print(classification_report(y_test, predic_rf))

"""Tahapan di atas menggambarkan proses pelatihan dan evaluasi model klasifikasi menggunakan algoritma Random Forest."""

# Confusion Matrix

rm_cm = confusion_matrix(y_test, predic_rf)
rm_cm = metrics.ConfusionMatrixDisplay(confusion_matrix=rm_cm)
rm_cm.plot()

"""## Decision Tree"""

# Decision Tree

model_dt = DecisionTreeClassifier()
model_dt.fit(X_train, y_train)
predic_dt = model_dt.predict(X_test)

print("akurasi:", accuracy_score(y_test, predic_dt))
print(classification_report(y_test, predic_dt))

"""Tahapan di atas menjelaskan proses pelatihan dan evaluasi model klasifikasi menggunakan Decision Tree"""

# Confusion Matrix

dt_cm = confusion_matrix(y_test, predic_dt)
dt_cm = metrics.ConfusionMatrixDisplay(confusion_matrix=dt_cm)
dt_cm.plot()

"""## KNN model"""

# KNN model

model_knn = KNeighborsClassifier(n_neighbors=5, algorithm='brute', weights='uniform')
model_knn.fit(X_train, y_train)
predic_knn = model_knn.predict(X_test)

print("akurasi:", accuracy_score(y_test, predic_knn))
print(classification_report(y_test, predic_knn))

"""Tahapan di atas menjelaskan proses pelatihan dan evaluasi model klasifikasi menggunakan K-Nearest Neighbors (KNN)"""

# Confusion Matrix

knn_cm = confusion_matrix(y_test, predic_knn)
knn_cm = metrics.ConfusionMatrixDisplay(confusion_matrix=knn_cm)
knn_cm.plot()

"""## AdaBoost"""

# AdaBoost

model_ab = AdaBoostClassifier(estimator=LogisticRegression(random_state=42),
                              n_estimators=50,
                              random_state=42)
model_ab.fit(X_train, y_train)
predic_ab = model_ab.predict(X_test)

print("akurasi:", accuracy_score(y_test, predic_ab))
print(classification_report(y_test, predic_ab))

"""Tahapan di atas menjelaskan proses pelatihan dan evaluasi model klasifikasi menggunakan AdaBoost, dengan Logistic Regression sebagai estimator dasar"""

# Confusion Matrix

ab_cm = confusion_matrix(y_test, predic_ab)
ab_cm = metrics.ConfusionMatrixDisplay(confusion_matrix=ab_cm)
ab_cm.plot()

"""## Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE

# Scaling Data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Langkah 2: Menggunakan SMOTE untuk menangani ketidakseimbangan kelas
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# Langkah 3: Menentukan parameter grid untuk KNN
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15, 20],  # Jumlah tetangga terdekat
    'weights': ['uniform', 'distance'],  # Bobot tetangga
    'algorithm': ['brute', 'ball_tree', 'kd_tree', 'auto']  # Algoritma pencarian
}

# Langkah 4: Buat objek KNN
knn = KNeighborsClassifier()

# Langkah 5: Buat objek GridSearchCV
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid,
                           scoring='accuracy', cv=5, n_jobs=-1)

# Langkah 6: Latih model dengan GridSearchCV menggunakan data yang sudah disampling
grid_search.fit(X_train_resampled, y_train_resampled)

# Menampilkan hasil
print("Best Hyperparameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

# Langkah 7: Menggunakan model terbaik untuk prediksi
best_knn = grid_search.best_estimator_
predic_knn_best = best_knn.predict(X_test_scaled)

# Evaluasi model terbaik
print("Akurasi pada test set:", accuracy_score(y_test, predic_knn_best))
print(classification_report(y_test, predic_knn_best))

"""Setelah menggunakan hyperparameter tuning akurasi model meningkat dan memiliki akurasi yang lebih tinggi dari KKN model."""